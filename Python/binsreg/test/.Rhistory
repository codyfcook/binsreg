## Main dataset (t=1)
data1 <- data[data[,"t"]==1,];
x <- data1$x; w <- data1$w; y.true <- data1$y.true; y <- data1$y;
## Used to illustrate cov adj. (w indep of x)
w.x <- data1$w.x; y.wx <- data1$y.wx
## Second group dataset (t=2)
data2 <- data[data[,"t"]==2,];
x2 <- data2$x; w2 <- data2$w; y2.true <- data2$y.true; y2 <- data2$y;
## Specify knots
nbins <- 10
knots <- quantile(x, seq(0, 1, 1/nbins), names = F)
pdf.width <- 6
pdf.height <- 4.5
################################################################################
## Figure 1: Canonical Binscatter
################################################################################
fig <- ggplot() + theme_bw()
fig <- fig + geom_point(data=data1, aes(x=x, y=y), shape=16, col="lightgrey") +
geom_vline(xintercept=knots, colour="black", lty="dashed", lwd=0.2) +
ylim(12,22) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text.x=element_blank(), axis.text.y=element_blank(),
axis.ticks.x=element_blank(), axis.ticks.y=element_blank(),
legend.position="none") + labs(x="X", y="Y")
dots <- binsreg(y, x, nbins=10)$data.plot[[1]]$data.dots
fig <- fig + geom_point(data=dots, aes(x=x, y=fit), col="blue", size=2, shape=19)
fig
dots
################################################################################
## Figure 1: Canonical Binscatter
################################################################################
fig <- ggplot() + theme_bw()
fig <- fig + geom_point(data=data1, aes(x=x, y=y), shape=16, col="lightgrey") +
geom_vline(xintercept=knots, colour="black", lty="dashed", lwd=0.2) +
ylim(12,22)
dots <- binsreg(y, x, nbins=10)$data.plot[[1]]$data.dots
fig <- fig + geom_point(data=dots, aes(x=x, y=fit), col="blue", size=2, shape=19)
fig
dots
knots
seq(0, 1, 1/nbins)
setwd("~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1")
rm(list = ls())
library(glmnet)
setwd("~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1")
student_mat <- read_csv("student-por.csv")
setwd("~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1")
student_mat <- read.csv("student-por.csv")
rm(list = ls())
library(readr)
library(dplyr)
library(glmnet)
# Data load ---------------------------------------------------------------
setwd("~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1")
student_mat <- read_delim("student-por.csv",
";", escape_double = FALSE, trim_ws = TRUE)
students_mat
student_mat <- read_delim("student-por.csv", ";", escape_double = FALSE, trim_ws = TRUE)
# Estatistica descritivas -------------------------------------------------
str(student_mat)
summary(student_mat)
str(student_mat)
for (col in colnames(student_mat)) {
if(!is.numeric(student_mat[[col]])) {
aux <- factor(student_mat[[col]])
if(length(levels(aux)) != 2){
# cat(col, " has more than two levels\n" )
cat("Dropping", col, "\n" )
student_mat[col] = NULL
}
else{
student_mat[col] <- as.numeric(aux) -1
cat(sprintf("%s = 0\t%s = 1\n", levels(aux)[1], levels(aux)[2]))
}
}
}
student_mat <- student_mat %>% select(-G2,-G3)
student_mat
str(student_mat)
lambdas_to_try <- 10^seq(-3, 4, length.out = 100)
ridge_cv <- cv.glmnet(X, y,
alpha = 0,
lambda = lambdas_to_try,
standardize = FALSE,
nfolds = 10)
student_mat$G1
student_mat
students_mt
students_mat
student_mat
student_mat[-G1]
student_mat[-'G1']
student_mat['G1']
student_mat[,'G1']
student_mat[,-c('G1')]
student_mat[,- 'G1']
-which(names(mtcars) == "carb")
names(student_mat)
names(student_mat)=='G1'
names(student_mat)!=='G1'
names(student_mat)!='G1'
y <- student_mat$G1
X <- student_mat[,names(student_mat)!='G1']
names(student_mat)!='G1'
y
X
X
X
dim(X)
dim(student_mat)
y <- student_mat$G1
X <- student_mat[,names(student_mat)!='G1']
lambdas_to_try <- 10^seq(-3, 4, length.out = 100)
ridge_cv <- cv.glmnet(X, y,
alpha = 0,
lambda = lambdas_to_try,
standardize = FALSE,
nfolds = 10)
as.matrix(student_mat[,names(student_mat)!='G1'])
y <- as.matrix(student_mat$G1)
X <- as.matrix(student_mat[,names(student_mat)!='G1'])
X <- student_mat[,names(student_mat)!='G1']
y
type(y)
class(y)
class(X)
X
X
class(X)
as.matrix(student_mat[,names(student_mat)!='G1'])
X <- as.matrix(student_mat[,names(student_mat)!='G1'])
class(X)
# Ridge Regression - cross-validation -------------------------------------
lambdas_to_try <- 10^seq(-3, 4, length.out = 100)
ridge_cv <- cv.glmnet(X, y,
alpha = 0,
lambda = lambdas_to_try,
standardize = FALSE,
nfolds = 10)
# Plot cross-validation results
plot(ridge_cv)
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
model <- glmnet(X, y, alpha = 0, lambda = lambda_cv)
# Coeficientes do modelo
coef(model)
y_hat <- predict(model, X)
SSR <- t(y - y_hat) %*% (y - y_hat)
rsq_ridge <- cor(y, y_hat)^2
cat(sprintf("SSR: %5.3f\n R2: %5.3f", SSR, rsq_ridge))
source('~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1/Lista1-Q1.R')
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try)
plot(res, xvar = "lambda")
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try)
plot(res, xvar = "lambda")
lambdas_to_try <- 10^seq(-3, 4, length.out = 100)
ridge_cv <- cv.glmnet(X, y,
alpha = 1,
lambda = lambdas_to_try,
standardize = FALSE,
nfolds = 10)
# Plot cross-validation results
plot(ridge_cv)
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
model <- glmnet(X, y, alpha = 1, lambda = 1)
# Coeficientes do modelo
coef(model)
# Fitted values
y_hat <- predict(model, X)
SSR <- t(y - y_hat) %*% (y - y_hat)
rsq_ridge <- cor(y, y_hat)^2
cat(sprintf("SSR: %5.3f\n R2: %5.3f", SSR, rsq_ridge))
source('~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1/Lista1-Q1.R')
# # Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:
# 1 school - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)
# 2 sex - student's sex (binary: "F" - female or "M" - male)
# 3 age - student's age (numeric: from 15 to 22)
# 4 address - student's home address type (binary: "U" - urban or "R" - rural)
# 5 famsize - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)
# 6 Pstatus - parent's cohabitation status (binary: "T" - living together or "A" - apart)
# 7 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
# 8 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
# 9 Mjob - mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
# 10 Fjob - father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
# 11 reason - reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")
# 12 guardian - student's guardian (nominal: "mother", "father" or "other")
# 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
# 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
# 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)
# 16 schoolsup - extra educational support (binary: yes or no)
# 17 famsup - family educational support (binary: yes or no)
# 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
# 19 activities - extra-curricular activities (binary: yes or no)
# 20 nursery - attended nursery school (binary: yes or no)
# 21 higher - wants to take higher education (binary: yes or no)
# 22 internet - Internet access at home (binary: yes or no)
# 23 romantic - with a romantic relationship (binary: yes or no)
# 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
# 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)
# 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)
# 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
# 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
# 29 health - current health status (numeric: from 1 - very bad to 5 - very good)
# 30 absences - number of school absences (numeric: from 0 to 93)
#
# # these grades are related with the course subject, Math or Portuguese:
# 31 G1 - first period grade (numeric: from 0 to 20)
# 31 G2 - second period grade (numeric: from 0 to 20)
# 32 G3 - final grade (numeric: from 0 to 20, output target)
#
# Additional note: there are several (382) students that belong to both datasets .
# These students can be identified by searching for identical attributes
# that characterize each student, as shown in the annexed R file.
# Setup -------------------------------------------------------------------
rm(list = ls())
library(readr)
library(dplyr)
library(glmnet)
# Data load ---------------------------------------------------------------
setwd("~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1")
student_mat <- read_delim("student-por.csv", ";", escape_double = FALSE, trim_ws = TRUE)
# Estatistica descritivas -------------------------------------------------
str(student_mat)
summary(student_mat)
# Convert all variables to numeric -----------------------------------------
for (col in colnames(student_mat)) {
if(!is.numeric(student_mat[[col]])) {
aux <- factor(student_mat[[col]])
if(length(levels(aux)) != 2){
# cat(col, " has more than two levels\n" )
cat("Dropping", col, "\n" )
student_mat[col] = NULL
}
else{
student_mat[col] <- as.numeric(aux) -1
cat(sprintf("%s = 0\t%s = 1\n", levels(aux)[1], levels(aux)[2]))
}
}
}
student_mat <- student_mat %>% select(-G2,-G3)
# Define Reponse Variable and Regressors ----------------------------------
y <- as.matrix(student_mat$G1)
X <- as.matrix(student_mat[,names(student_mat)!='G1'])
# Ridge Regression - cross-validation -------------------------------------
lambdas_to_try <- 10^seq(-3, 4, length.out = 100)
ridge_cv <- cv.glmnet(X, y,
alpha = 0,
lambda = lambdas_to_try,
standardize = FALSE,
nfolds = 10)
# Plot cross-validation results
plot(ridge_cv)
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
# Ridge Regression - coeficientes, SSR e R2 -------------------------------
model <- glmnet(X, y, alpha = 0, lambda = lambda_cv)
# Coeficientes do modelo
coef(model)
# Fitted values
y_hat <- predict(model, X)
SSR <- t(y - y_hat) %*% (y - y_hat)
rsq <- cor(y, y_hat)^2
cat(sprintf("SSR: %5.3f\n R2: %5.3f", SSR, rsq))
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try)
plot(res, xvar = "lambda")
# Lasso Regression - cross-validation -------------------------------------
lambdas_to_try <- 10^seq(-3, 4, length.out = 100)
ridge_cv <- cv.glmnet(X, y,
alpha = 1,
lambda = lambdas_to_try,
standardize = FALSE,
nfolds = 10)
Plot cross-validation results
plot(ridge_cv)
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
idge_cv <- cv.glmnet(X, y,
alpha = 1,
lambda = lambdas_to_try,
standardize = FALSE,
nfolds = 10)
plot(ridge_cv)
lambda_cv <- ridge_cv$lambda.min
model <- glmnet(X, y, alpha = 1, lambda = 1)
coef(model)
y_hat <- predict(model, X)
SSR <- t(y - y_hat) %*% (y - y_hat)
rsq <- cor(y, y_hat)^2
cat(sprintf("SSR: %5.3f\n R2: %5.3f", SSR, rsq))
res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try)
plot(res, xvar = "lambda")
library(broom)
library(ggplot2)
coef(model, s = "lambda.1se") %>%
# tidy() %>%
as.matrix() %>%
as_tibble() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
source('~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1/Lista1-Q1.R')
data("crabs", package = "MASS")
rm(list = ls())
data("crabs", package = "MASS")
summary(crabs)
crabs %>% select(FL, RW, CL, CW, BD) %>% plot()
source('~/Dropbox/Teaching/EESP-FGV/Masters/ML-2020/Listas/Lista 1/Lista1-Q2.R')
crabs.test
crabs.train %>% select(sex, Previsao) %>% table()
pred1 <- predict(fit, type="response")
crabs.train$Previsao <- ifelse(pred1 > 0.5 , 'M', 'F')
crabs.train %>% select(sex, Previsao) %>% table()
pred2 <- predict(fit, newdata = crabs.test)
crabs.test$Previsao <- ifelse(pred2 > 0.5 ,  'M', 'F')
crabs.test %>% select(sex, Previsao) %>% table()
source('~/.active-rstudio-document')
data <- read.csv("binsreg_sim.csv", sep=",")
setwd("~/Dropbox/binsreg/Python/binsreg/test")
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
bins <- binsregselect(y, x, data=data, bins = c(3,3))
summary(bins)
bins <- binsregselect(y, x, data=data, bins = c(4,4))
summary(bins)
source('~/.active-rstudio-document')
genB
source('~/Dropbox/binsreg/R/binsreg/R/binsreg_functions.R')
source('~/Dropbox/binsreg/R/binsreg/R/binsreg_functions.R')
setwd("~/Dropbox/binsreg/Python/binsreg/test")
data <- read.csv("binsreg_sim.csv", sep=",")
y <- data$y; x <- data$x; w <- data$w; t <- data$t; id <- data$id; d <- data$d
summary(data)
knot = genKnot.qs(x, J=4)
knot
out = genB(y, x, w = NULL, p=3, s=3, deriv=0, knot = knot, weights=NULL)
library(splines)
out = genB(y, x, w = NULL, p=3, s=3, deriv=0, knot = knot, weights=NULL)
out
B  <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=0)
p =3
s= 3
p
s
x
y
w
w = None
w = NULL
knot = genKnot_qs(x, J=4)
knot = genKnot.qs(x, J=4)
knot
B  <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=0)
k    <- ncol(B)
k
P    <- cbind(B, w)
P
dim(P)
beta <- lm(y~P-1, weights=weights)$coefficients[1:k]
beta <- lm(y~P-1, weights=1)$coefficients[1:k]
beta <- lm(y~P-1)$coefficients[1:k]
beta
pos  <- !is.na(beta)
pos
basis <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=p+1)
mu.m.fit  <- basis[, pos, drop=F] %*% beta[pos]
mu.m.fit
setwd("~/Dropbox/binsreg/Python/binsreg/test")
data <- read.csv("binsreg_sim.csv", sep=",")
y <- data$y; x <- data$x; w <- data$w; t <- data$t; id <- data$id; d <- data$d
summary(data)
J = 4
knot = genKnot_qs(x, J)
J = 4
knot = genKnot.qs(x, J)
# Test bias function
# bias(x, p, s, deriv, knot)
p = 3
s = 3
deriv = 0
b = bias(x, p, s, deriv, knot)
b
b[1:10]
J = 4
knot = genKnot.qs(x, J)
w = NULL
# Test bias function
# bias(x, p, s, deriv, knot)
p = 3
s = 3
deriv = 0
B  <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=0)   # use smoothest spline
k    <- ncol(B)
P    <- cbind(B, w)
beta <- lm(y~P-1, weights=weights)$coefficients[1:k]
pos  <- !is.na(beta)
basis <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=p+1)
mu.m.fit  <- basis[, pos, drop=F] %*% beta[pos]
bias.0 <- mu.m.fit * bias(x, p, s, 0, knot)    # proj component, v=0!!!
B    <- binsreg.spdes(eval=x, p=p, s=s, knot=knot, deriv=0)
beta <- lm(bias.0~B-1, weights=weights)$coefficients
pos <- !is.na(beta)
if (deriv > 0) {
basis <- binsreg.spdes(eval=x, p=p, s=s, knot=knot, deriv=deriv)
bias.v <- mu.m.fit * bias(x, p, s, deriv, knot)    # need to recalculate for v>0!!!
} else {
basis <- B
bias.v <- bias.0
}
bias.l2 <- bias.v - basis[,pos,drop=F] %*% beta[pos]
bias.cons <- binsreg.summ(bias.l2^2, w=weights, std=F)$mu
B
k
P
beta
pos
basis
mu.m.fit  <- basis[, pos, drop=F] %*% beta[pos]
B  <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=0)
beta <- lm(y~P-1, weights=weights)$coefficients[1:k]
beta <- lm(y~P-1)$coefficients[1:k]
pos  <- !is.na(beta)
basis <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=p+1)
mu.m.fit  <- basis[, pos, drop=F] %*% beta[pos]
mu_m_fit[1:10]
mu.m.fit[1:10]
mu.m.fit[990:1000]
mu.m.fit[991:1000]
mu.m.fit[991:1000]
bias.0 <- mu.m.fit * bias(x, p, s, 0, knot)
bias0[1:10,:]
bias.0[1:10,:]
bias.0[1:10]
B    <- binsreg.spdes(eval=x, p=p, s=s, knot=knot, deriv=0)
B.shape
dim(B)
m(bias.0~B-1)$coefficients
lm(bias.0~B-1)$coefficients
beta <- lm(bias.0~B-1, weights=weights)$coefficients
beta <- lm(bias.0~B-1)$coefficients
beta
pos <- !is.na(beta)
pos
basis <- B
bias.v <- bias.0
bias.l2 <- bias.v - basis[,pos,drop=F] %*% beta[pos]
bias.l2
bias.l2[1:10]
weights = 1
B  <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=0)   # use smoothest spline
k    <- ncol(B)
P    <- cbind(B, w)
beta <- lm(y~P-1, weights=weights)$coefficients[1:k]
pos  <- !is.na(beta)
basis <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=p+1)
mu.m.fit  <- basis[, pos, drop=F] %*% beta[pos]
bias.0 <- mu.m.fit * bias(x, p, s, 0, knot)    # proj component, v=0!!!
B    <- binsreg.spdes(eval=x, p=p, s=s, knot=knot, deriv=0)
beta <- lm(bias.0~B-1, weights=weights)$coefficients
pos <- !is.na(beta)
if (deriv > 0) {
basis <- binsreg.spdes(eval=x, p=p, s=s, knot=knot, deriv=deriv)
bias.v <- mu.m.fit * bias(x, p, s, deriv, knot)    # need to recalculate for v>0!!!
} else {
basis <- B
bias.v <- bias.0
}
bias.l2 <- bias.v - basis[,pos,drop=F] %*% beta[pos]
weights = rep(1,1000)
B  <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=0)   # use smoothest spline
k    <- ncol(B)
P    <- cbind(B, w)
beta <- lm(y~P-1, weights=weights)$coefficients[1:k]
pos  <- !is.na(beta)
basis <- binsreg.spdes(eval=x, p=p+1, s=s+1, knot=knot, deriv=p+1)
mu.m.fit  <- basis[, pos, drop=F] %*% beta[pos]
bias.0 <- mu.m.fit * bias(x, p, s, 0, knot)    # proj component, v=0!!!
B    <- binsreg.spdes(eval=x, p=p, s=s, knot=knot, deriv=0)
beta <- lm(bias.0~B-1, weights=weights)$coefficients
pos <- !is.na(beta)
if (deriv > 0) {
basis <- binsreg.spdes(eval=x, p=p, s=s, knot=knot, deriv=deriv)
bias.v <- mu.m.fit * bias(x, p, s, deriv, knot)    # need to recalculate for v>0!!!
} else {
basis <- B
bias.v <- bias.0
}
bias.l2 <- bias.v - basis[,pos,drop=F] %*% beta[pos]
bias.l2
bias.l2[1:10]
binsreg.summ(bias.l2^2, w=weights, std=F)$mu
bias.l2
bias.l2[1:10]
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
bins = binsregselect(y, x, data=data, bins = c(4,2)
summary(bins)
bins = binsregselect(y, x, data=data, bins = c(4,2))
summary(bins)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
